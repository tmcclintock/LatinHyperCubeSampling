{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/lhs/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: Unsupported `ReduceOp` for distributed computing.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from pyDOE2 import lhs\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import flatten, nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as tvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = tvd.FashionMNIST(root=\"./data/\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_ds = tvd.FashionMNIST(root=\"./data/\", train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = int(0.75 * len(train_full))\n",
    "train_ds, val_ds = torch.utils.data.random_split(train_full, [cut, len(train_full) - cut])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds , batch_size=32, num_workers=4)\n",
    "val_dl = DataLoader(train_ds, batch_size=32, num_workers=2)\n",
    "test_dl = DataLoader(test_ds, batch_size=32, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here: sample the LH for hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMNISTClassifier(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(FMNISTClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.conv1(x)\n",
    "        #x = self.MP1(x)\n",
    "        #x = self.conv2(x)\n",
    "        #x = self.MP2(x)\n",
    "        x = flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x  # self.fc2(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        loss = F.cross_entropy(self(x), y)\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        loss = F.cross_entropy(self(x), y)\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "        return {\"val_loss\": avg_loss, \"log\": tensorboard_logs}\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        loss = F.cross_entropy(self(x), y)\n",
    "        return {\"test_loss\": loss}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'test_loss': avg_loss}\n",
    "        return {\"test_loss\": avg_loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.SGD(self.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | fc1  | Linear | 7 K   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Validation sanity check: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/lhs/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/2814 [00:00<?, ?it/s] \n",
      "Epoch 1:   0%|          | 0/2814 [00:07<?, ?it/s, loss=0.938, v_num=0]\n",
      "Epoch 2:   0%|          | 0/2814 [00:00<?, ?it/s, loss=0.938, v_num=0]\n",
      "Epoch 2:   0%|          | 0/2814 [00:06<?, ?it/s, loss=0.844, v_num=0]\n",
      "Epoch 3:   0%|          | 0/2814 [00:00<?, ?it/s, loss=0.844, v_num=0]\n",
      "Epoch 3:   0%|          | 0/2814 [00:06<?, ?it/s, loss=0.802, v_num=0]\n",
      "Epoch 4:   0%|          | 0/2814 [00:00<?, ?it/s, loss=0.802, v_num=0]\n",
      "Epoch 4:   0%|          | 0/2814 [00:07<?, ?it/s, loss=0.775, v_num=0]\n",
      "Epoch 5:   0%|          | 0/2814 [00:00<?, ?it/s, loss=0.775, v_num=0]\n",
      "Epoch 5:   0%|          | 0/2814 [00:07<?, ?it/s, loss=0.756, v_num=0]\n",
      "Epoch 6:   0%|          | 0/2814 [00:00<?, ?it/s, loss=0.756, v_num=0]\n",
      "Epoch 6:   0%|          | 0/2814 [00:07<?, ?it/s, loss=0.742, v_num=0]\n",
      "Epoch 7:   0%|          | 0/2814 [00:00<?, ?it/s, loss=0.742, v_num=0]\n",
      "Epoch 7:   0%|          | 0/2814 [00:07<?, ?it/s, loss=0.731, v_num=0]\n",
      "Epoch 8:   0%|          | 0/2814 [00:00<?, ?it/s, loss=0.731, v_num=0]\n",
      "Epoch 8:   0%|          | 0/2814 [00:07<?, ?it/s, loss=0.722, v_num=0]\n",
      "Epoch 9:   0%|          | 0/2814 [00:00<?, ?it/s, loss=0.722, v_num=0]\n",
      "Epoch 9:   0%|          | 0/2814 [00:07<?, ?it/s, loss=0.514, v_num=0]\n",
      "Epoch 10:   0%|          | 0/2814 [00:00<?, ?it/s, loss=0.514, v_num=0]\n",
      "Epoch 10:   0%|          | 0/2814 [00:07<?, ?it/s, loss=0.499, v_num=0]\n",
      "Epoch 11:   0%|          | 0/2814 [00:00<?, ?it/s, loss=0.499, v_num=0]\n",
      "Epoch 11:   0%|          | 0/2814 [00:07<?, ?it/s, loss=0.490, v_num=0]\n",
      "Epoch 12:   0%|          | 0/2814 [00:00<?, ?it/s, loss=0.490, v_num=0]\n",
      "Epoch 12:   0%|          | 0/2814 [00:07<?, ?it/s, loss=0.483, v_num=0]\n",
      "Epoch 13:   0%|          | 0/2814 [00:00<?, ?it/s, loss=0.483, v_num=0]\n",
      "Epoch 13:   0%|          | 0/2814 [00:07<?, ?it/s, loss=0.478, v_num=0]\n",
      "Epoch 14:   0%|          | 0/2814 [00:00<?, ?it/s, loss=0.478, v_num=0]\n",
      "Epoch 14:   0%|          | 0/2814 [00:07<?, ?it/s, loss=0.474, v_num=0]\n",
      "Epoch 15:   0%|          | 0/2814 [00:00<?, ?it/s, loss=0.474, v_num=0]\n",
      "Epoch 15:   0%|          | 0/2814 [00:07<?, ?it/s, loss=0.470, v_num=0]\n",
      "Epoch 16:   0%|          | 0/2814 [00:00<?, ?it/s, loss=0.470, v_num=0]\n",
      "Epoch 16:   0%|          | 0/2814 [00:07<?, ?it/s, loss=0.466, v_num=0]\n",
      "Epoch 17:   0%|          | 0/2814 [00:00<?, ?it/s, loss=0.466, v_num=0]\n",
      "Epoch 17:   0%|          | 0/2814 [00:06<?, ?it/s, loss=0.463, v_num=0]\n",
      "Epoch 18:   0%|          | 0/2814 [00:00<?, ?it/s, loss=0.463, v_num=0]\n",
      "Epoch 18:   0%|          | 0/2814 [00:06<?, ?it/s, loss=0.461, v_num=0]\n",
      "Epoch 19:   0%|          | 0/2814 [00:00<?, ?it/s, loss=0.461, v_num=0]\n",
      "Epoch 19:   0%|          | 0/2814 [00:06<?, ?it/s, loss=0.458, v_num=0]\n",
      "Epoch 20:   0%|          | 0/2814 [00:00<?, ?it/s, loss=0.458, v_num=0]\n",
      "Epoch 20:   0%|          | 0/2814 [00:06<?, ?it/s, loss=0.456, v_num=0]\n",
      "Epoch 20:   0%|          | 0/2814 [00:06<?, ?it/s, loss=0.456, v_num=0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist_model = FMNISTClassifier()\n",
    "trainer = pl.Trainer(progress_bar_refresh_rate=3000, max_epochs=20)\n",
    "trainer.fit(fmnist_model, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Testing: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/lhs/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'test_loss': tensor(0.4754)}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing:   0%|          | 0/313 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.4754374921321869}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(fmnist_model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 11108), started 0:49:32 ago. (Use '!kill 11108' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ac2a0b6371fad412\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ac2a0b6371fad412\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "loglrs = [-4, -5, -6]\n",
    "loss_record = {}\n",
    "for loglr in loglrs:\n",
    "    lr = 10**loglr\n",
    "    mom = 0.9\n",
    "    loss_record[(loglr, mom)] = perform_training(lr, mom, num_epochs=20)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for (loglr, mom), losses in loss_record.items():\n",
    "    plt.loglog(np.arange(len(losses))+1, losses, label=loglr)\n",
    "plt.legend()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
